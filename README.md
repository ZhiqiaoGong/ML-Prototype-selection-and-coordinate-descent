# Project 1: Prototype Selection for Nearest Neighbor
This project explored various prototype selection methods—Random, K-Means, and Condensed Nearest Neighbor (CNN)—to enhance the efficiency of the nearest neighbor classifier using the MNIST dataset. The methods were assessed based on their test error and computational runtime across different prototype sizes ranging from 100 to 10,000. Statistical analyses were conducted to determine the mean test errors with 95% confidence intervals. The findings indicated trade-offs between accuracy and computational efficiency, with K-Means generally showing lower test errors at larger prototype sizes, while the Random method offered a balance between runtime efficiency and accuracy. Advanced strategies like combining prototype selection methods or employing machine learning enhancements were proposed for future exploration.

# Project 2: Coordinate Descent
This project focused on evaluating various coordinate descent methods—random, cyclic, and greedy—integrated with a learning rate decay for optimizing logistic regression. The methods were compared over 3000 iterations to assess their performance in terms of convergence speed and logistic loss reduction. Cyclic coordinate descent was found to be the most effective, achieving the fastest convergence and the lowest logistic loss. The study also considered potential enhancements for coordinate descent techniques, such as adaptive learning rate adjustments and hybrid coordinate selection strategies, to optimize their performance in binary classification tasks. The results demonstrated that these methods are viable alternatives to standard logistic regression, especially when enhanced by learning rate adjustments and sophisticated termination criteria.






